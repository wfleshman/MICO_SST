{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Competition (MICO) @ IEEE SatML 2023: SST-2\n",
    "\n",
    "Welcome to the MICO competition!\n",
    "\n",
    "This notebook will walk you through the process of creating and packaging a submission to one of the challenges.\n",
    "\n",
    "Let's start by downloading and extracting the archives for the SST-2 challenge.\n",
    "We split the challenge data into three archives, one per scenario (~80GiB each).\n",
    "Downloading, verifying, and extracting them can take a while, so you may want to run the cell below only once.\n",
    "\n",
    "**NOTE**: Public anonymous access to the competition data is disabled. \n",
    "Upon registering for the competition, you will be shown URLs with embedded bearer tokens that you must use instead of the URLs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "files = [\n",
    "    {\n",
    "        'filename' : 'sst2_lo.tar.gz',\n",
    "        'url': 'FILL_IN_WITH_CORRECT',\n",
    "        'md5': '205414dd0217065dcebe2d8adc1794a3'\n",
    "    },\n",
    "    {\n",
    "        'filename' : 'sst2_hi.tar.gz',\n",
    "        'url': 'FILL_IN_WITH_CORRECT',\n",
    "        'md5': 'd285958529fcad486994347478feccd2'\n",
    "    },\n",
    "    {\n",
    "        'filename' : 'sst2_inf.tar.gz',\n",
    "        'url': 'FILL_IN_WITH_CORRECT',\n",
    "        'md5': '7dca44b191a0055edbde5c486a8fc671'\n",
    "    }\n",
    "]\n",
    "\n",
    "# WARNING: this will download and extract three ~80GiB files, if not already present. Please save the files and avoid re-downloading them.\n",
    "try:\n",
    "    for f in files:\n",
    "        url, filename, md5 = f['url'], f['filename'], f['md5']\n",
    "        print(f\"Downloading and extracting {filename}...\")\n",
    "        download_and_extract_archive(url=url, download_root=os.curdir, extract_root=None, filename=filename, md5=md5, remove_finished=False)\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e)\n",
    "    print(\"Have you replaced the URLs above with the one you got after registering?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from mico_competition import ChallengeDataset, load_sst2, load_model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available; the below would only work with CUDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(D, tokenizer, max_sequence_length):\n",
    "    processed_data = D.map(\n",
    "        lambda batch: tokenizer(batch[\"sentence\"], padding=\"max_length\", max_length=max_sequence_length),\n",
    "        batched=True\n",
    "    )\n",
    "    return processed_data.remove_columns([\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rest_points):\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    device = 'cuda'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=2).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    ds = datasets.DatasetDict({\n",
    "        'train': datasets.Dataset.from_pandas(\n",
    "        pd.DataFrame.from_records(rest_points))}).remove_columns(\"idx\")\n",
    "    ds = preprocess_text(ds, tokenizer, 67)\n",
    "    model.train()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/tmp',\n",
    "        lr_scheduler_type= 'constant',\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        save_strategy='no',\n",
    "        dataloader_num_workers=8,\n",
    "        per_device_train_batch_size=96,\n",
    "        gradient_accumulation_steps=1)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        args = training_args,\n",
    "        train_dataset = ds['train'],\n",
    "        model = model,\n",
    "        tokenizer = tokenizer\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHALLENGE = \"sst2\"\n",
    "LEN_TRAINING = 67349\n",
    "LEN_CHALLENGE = 100\n",
    "\n",
    "scenarios = os.listdir(CHALLENGE)\n",
    "\n",
    "dataset = load_sst2()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "scenario = 'sst2_hi'\n",
    "train_path = os.path.join(CHALLENGE, scenario, 'train')\n",
    "\n",
    "for m, model_folder in enumerate(tqdm(sorted(os.listdir(train_path), key=lambda d: int(d.split('_')[1])), desc=\"model\")):\n",
    "    data_path = os.path.join(train_path, model_folder)\n",
    "    challenge_dataset = ChallengeDataset.from_path(data_path, dataset=dataset, len_training=LEN_TRAINING)\n",
    "    rest_points = challenge_dataset.rest\n",
    "    ref_model = train(rest_points)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_points = challenge_dataset.get_challenges()\n",
    "challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=10)\n",
    "\n",
    "y = np.loadtxt(os.path.join(data_path, \"solution.csv\"),   delimiter=\",\")\n",
    "model_path = os.path.join(train_path, model_folder)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = load_model('sst2', model_path).eval().cuda()\n",
    "    preds = []\n",
    "    ref_preds = []\n",
    "    for batch in challenge_dataloader:\n",
    "        labels = batch['label'].to(torch.device('cuda'))\n",
    "        tokenizedSequences = tokenizer(batch['sentence'], return_tensors=\"pt\", padding=\"max_length\", max_length=67)\n",
    "        tokenizedSequences = tokenizedSequences.to(torch.device('cuda'))\n",
    "        \n",
    "        # query model\n",
    "        output = model(**tokenizedSequences)\n",
    "        batch_predictions = F.softmax(output.logits, dim=1)[torch.arange(output.logits.shape[0]), labels].cpu().numpy()\n",
    "        preds.extend(batch_predictions)\n",
    "    \n",
    "        # ref model\n",
    "        output = ref_model(**tokenizedSequences)\n",
    "        batch_predictions = F.softmax(output.logits, dim=1)[torch.arange(output.logits.shape[0]), labels].cpu().numpy()\n",
    "        ref_preds.extend(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(preds) - np.array(ref_preds)\n",
    "scores = (scores - scores.min()) / (scores.max() - scores.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores[y==0], bins=30, color='blue', alpha=0.5)\n",
    "plt.hist(scores[y==1], bins=30, color='red', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mico_competition.scoring import score\n",
    "\n",
    "score(y, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_path = os.path.join(CHALLENGE, scenario, 'dev')\n",
    "for m, model_folder in enumerate(tqdm(sorted(os.listdir(dev_path), key=lambda d: int(d.split('_')[1])), desc=\"model\")):\n",
    "    data_path = os.path.join(dev_path, model_folder)\n",
    "    challenge_dataset = ChallengeDataset.from_path(data_path, dataset=dataset, len_training=LEN_TRAINING)\n",
    "    challenge_points = challenge_dataset.get_challenges()\n",
    "    challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=10)\n",
    "    \n",
    "    rest_points = challenge_dataset.rest\n",
    "    ref_model = train(rest_points)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model = load_model('sst2', data_path).eval().cuda()\n",
    "        preds = []\n",
    "        ref_preds = []\n",
    "        for batch in challenge_dataloader:\n",
    "            labels = batch['label'].to(torch.device('cuda'))\n",
    "            tokenizedSequences = tokenizer(batch['sentence'], return_tensors=\"pt\", padding=\"max_length\", max_length=67)\n",
    "            tokenizedSequences = tokenizedSequences.to(torch.device('cuda'))\n",
    "\n",
    "            # query model\n",
    "            output = model(**tokenizedSequences)\n",
    "            batch_predictions = F.softmax(output.logits, dim=1)[torch.arange(output.logits.shape[0]), labels].cpu().numpy()\n",
    "            preds.extend(batch_predictions)\n",
    "\n",
    "            # ref model\n",
    "            output = ref_model(**tokenizedSequences)\n",
    "            batch_predictions = F.softmax(output.logits, dim=1)[torch.arange(output.logits.shape[0]), labels].cpu().numpy()\n",
    "            ref_preds.extend(batch_predictions)\n",
    "            \n",
    "    scores = np.array(preds) - np.array(ref_preds)\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "    \n",
    "    with open(os.path.join(data_path, \"prediction.csv\"), \"w\") as f:\n",
    "        csv.writer(f).writerow(list(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_path = os.path.join(CHALLENGE, scenario, 'final')\n",
    "for m, model_folder in enumerate(tqdm(sorted(os.listdir(final_path), key=lambda d: int(d.split('_')[1])), desc=\"model\")):\n",
    "    data_path = os.path.join(final_path, model_folder)\n",
    "    challenge_dataset = ChallengeDataset.from_path(data_path, dataset=dataset, len_training=LEN_TRAINING)\n",
    "    challenge_points = challenge_dataset.get_challenges()\n",
    "    challenge_dataloader = torch.utils.data.DataLoader(challenge_points, batch_size=10)\n",
    "    \n",
    "    rest_points = challenge_dataset.rest\n",
    "    ref_model = train(rest_points)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model = load_model('sst2', data_path).eval().cuda()\n",
    "        preds = []\n",
    "        ref_preds = []\n",
    "        for batch in challenge_dataloader:\n",
    "            labels = batch['label'].to(torch.device('cuda'))\n",
    "            tokenizedSequences = tokenizer(batch['sentence'], return_tensors=\"pt\", padding=\"max_length\", max_length=67)\n",
    "            tokenizedSequences = tokenizedSequences.to(torch.device('cuda'))\n",
    "\n",
    "            # query model\n",
    "            output = model(**tokenizedSequences)\n",
    "            batch_predictions = F.softmax(output.logits, dim=1)[torch.arange(output.logits.shape[0]), labels].cpu().numpy()\n",
    "            preds.extend(batch_predictions)\n",
    "\n",
    "            # ref model\n",
    "            output = ref_model(**tokenizedSequences)\n",
    "            batch_predictions = F.softmax(output.logits, dim=1)[torch.arange(output.logits.shape[0]), labels].cpu().numpy()\n",
    "            ref_preds.extend(batch_predictions)\n",
    "            \n",
    "    scores = np.array(preds) - np.array(ref_preds)\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "    \n",
    "    with open(os.path.join(data_path, \"prediction.csv\"), \"w\") as f:\n",
    "        csv.writer(f).writerow(list(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "phases = ['dev', 'final']\n",
    "scenarios = ['sst2_inf', 'sst2_lo', 'sst2_hi']\n",
    "\n",
    "with zipfile.ZipFile(\"predictions_sst2.zip\", 'w') as zipf:\n",
    "    for scenario in tqdm(scenarios, desc=\"scenario\"): \n",
    "        for phase in tqdm(phases, desc=\"phase\"):\n",
    "            root = os.path.join(CHALLENGE, scenario, phase)\n",
    "            for model_folder in tqdm(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])), desc=\"model\"):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    zipf.write(file)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}. You need to provide predictions for all challenges\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c823568a0650a753a55947c22141ec594c2fc02bd68b5a71e505ecc57f17796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
